# veScale: A PyTorch Native LLM Training Framework

## TLDR

<img src="../../public/guide/tldr.png" alt="TL'DR" width="450"/>

(`*` is under development.)

## Why veScale

The era of giant models today calls forth distributed training.
Despite countless distributed training frameworks that have been published in the past decade (to name a few), few have excelled at the _**Ease of Use**_ and development extensibility demanded by real industry production,
as the quality most favored for a framework is often the _**Ease of Use**_ instead of pure _Performance_. 
Companies developing 100s~1000s models a week benefit the most from a framework that is both easy to use and extend, and provides elegant encapsulation of models and clean APIs.

The _**Ease of Use**_ of a framework for training and developing LLM lies in the following essentials:

- üî• **PyTorch Native**: _PyTorch_ ecosystem dominates the ML world and owns 92% of models on _HuggingFace_ and 70% of research on _Papers with Code_; Alienating from _PyTorch_ ecosystem makes a framework hard to adapt and extend.

- üõ° **Zero Model Code Change**: Users' model code should remain untouched, instead of being intertwined with framework code, which requires users to not only manually rewrite the model for distributed training with tons of care, but also painfully debug within the deep coupled model and framework code.

- üöÄ **Single Device Abstraction**: Model developers should focus on developing model architecture itself with single device semantics, rather than being distracted by the complex and error-prone management of multiple devices and diverse interconnects in distributed environments.

- üéØ **Automatic Parallelism Planning***: Gigantic models cannot be trained without _nD Parallelism_ (_tensor, sequence, data, ZeRO, pipeline parallelism, etc._). Users' giant models should be automatically scaled by a framework for _nD_ parallel training, instead of being manually planned and tuned for each operator or layer under different cluster settings, which takes forever. 

- ‚ö° **Eager & Compile Mode***: Users should enjoy both _Eager_ and _Compile_ mode offered by a framework with:
  - _Eager_ mode for fast development, convenient debugging, and customization with callbacks and control flows;
  - _Compile_ mode for ultimate performance boost with a single click.

- üåê **Open Source & Reproducibility**: a great framework should be open-source, ensuring easy access and utilization for companies and research institutions alike.

## What is veScale

We take an initial step to develop an industry-level framework, **veScale**, that focuses _**Ease of Use**_ for scaling LLM training, by combining _PyTorch Nativeness_ and _Automatic Parallelism*_. 

Ideally, **veScale** only expects model developers to write a simple model code with native _torch.nn.Module_ under _Zero Code Change_ as if running on a _Single Device_, and then **veScale** will automatically parallelize it across a cluster of devices in a _nD Parallelism_ search space with all the optimizations and heavy lifting handled transparently.

Unlike existing frameworks that rely on _Compile_ mode and a "perfect model graph" for _Automatic Parallelism_,  **veScale** is inventing an _Eager-Mode-ONLY*_ _Automatic Parallelism_ that does not rely on the model graph at all. 
Furthermore, **veScale** is also developing a _Mixed Mode_* of partial _Eager_ and partial _Compile_.

**veScale**'s overview is as follows:

<img src="../../public/guide/overview.png" alt="overview" width="1000"/>

**veScale** is designed and implemented on top of a primitive called _DTensor_ that provides a global tensor semantic with local shards distributed on multiple devices.
**veScale** extends and enhances the _PyTorch DTensor_ for our production standard, and further develops the _Auto-Plan*_ and _Auto-Paralleize_ with a unified configuration and API. 
Furthermore, **veScale** also supports _Auto-Reshard_ for distributed checkpoints, which will be open-sourced as a new project -- **OmniStore**.

(`*` is under development)

## Status of veScale

**veScale** is still in its early phase.

The tentative open-source timeline is as follows:

- By mid April, 4D parallelism (tensor parallelism, sequence parallelism, data parallelism and ZERO) examples for nanoGPT, Llama2 and Mixtral models

- By end of May, fast checkpointing system

- By end of July, CUDA event monitor, pipeline parallelism and supporting components for large-scale training

## [License](https://github.com/volcengine/veScale/blob/main/LICENSE)

The veScale Project is under the Apache License v2.0.

## Acknowledgement

We would like to acknowledge the assistance of and collaboration with
the [PyTorch DTensor team](https://github.com/pytorch/pytorch/tree/main/torch/distributed/_tensor).

## [We are hiring!](https://volcengine.github.io/veScaleWeb/misc/join-us.html)